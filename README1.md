# Sampling on Imbalanced Dataset — Credit Card Fraud (Colab + GitHub)

## 1) Objective
This assignment studies **how different sampling strategies affect model performance** on an **imbalanced binary classification dataset**.  
We evaluate **5 sampling techniques × 5 ML models** using **5-fold stratified cross-validation**, and report **Accuracy** for every combination.

---

## 2) Dataset
- **File:** `Creditcard_data.csv`
- **Target column:** `Class` (binary label)
- **Features:** all remaining columns (numerical)

The dataset is imbalanced (fraud is the minority class), so training directly on raw data can bias models toward the majority class.

---

## 3) Methodology

### 3.1 Preprocessing
1. Load CSV into a dataframe.
2. Separate features `X` and target `y = Class`.
3. Handle any missing/invalid values (NaN/Inf) using median imputation.
4. Apply **Standard Scaling** only for models that are sensitive to feature scaling:
   - Logistic Regression
   - SVM
   - KNN

> Note: Scaling is done inside the pipeline to prevent leakage.

---

### 3.2 “Five Samples” (5-Fold Stratified Cross Validation)
To create five representative samples while preserving class ratio, we use:

- **StratifiedKFold(n_splits=5, shuffle=True, random_state=42)**

Each fold acts like a “sample.”  
For every sampler+model combo, we compute accuracy on all 5 folds and report the **mean accuracy**.

---

### 3.3 Sampling Techniques (Sampling1–Sampling5)
We test the following:

- **Sampling1 — Random Under-Sampling:** reduces majority class size by randomly removing samples.
- **Sampling2 — Random Over-Sampling:** duplicates minority samples to balance the class ratio.
- **Sampling3 — SMOTE:** synthesizes new minority samples using nearest neighbors.
- **Sampling4 — ADASYN:** like SMOTE, but generates more samples near difficult-to-learn regions.
- **Sampling5 — SMOTEENN:** combines SMOTE (oversample) + Edited Nearest Neighbors (clean noisy points).

All sampling happens **inside cross-validation folds** to avoid training/validation contamination.

---

### 3.4 Models (M1–M5)
We evaluate:

- **M1 — Logistic Regression**
- **M2 — Random Forest**
- **M3 — Gradient Boosting**
- **M4 — SVM (RBF Kernel)**
- **M5 — KNN**

---

### 3.5 Evaluation Metric
- **Primary metric:** **Accuracy**
- Accuracy is required by the assignment, but note:
  - On heavily imbalanced datasets, accuracy can be misleading.
  - (Optional) The notebook also prints confusion matrix / classification report for the best overall combo on a holdout split.

---

## 4) Results

### 4.1 Accuracy Table (5-fold mean Accuracy %)
The notebook generates this CSV:
- `sampling_model_accuracy_table.csv`

Paste the table output here after running (or embed it using the CSV-to-Markdown step below).

✅ **Accuracy Table (Models × Sampling Techniques):**
| Model \ Sampling | Sampling1 | Sampling2 | Sampling3 | Sampling4 | Sampling5 |
|---|---:|---:|---:|---:|---:|
| M1_LogReg | ... | ... | ... | ... | ... |
| M2_RandomForest | ... | ... | ... | ... | ... |
| M3_GradBoost | ... | ... | ... | ... | ... |
| M4_SVC | ... | ... | ... | ... | ... |
| M5_KNN | ... | ... | ... | ... | ... |

---

### 4.2 Best Sampling Technique per Model
The notebook generates:
- `best_sampler_per_model.csv`

✅ **Best sampler per model (highest accuracy):**
| Model | Best Sampling Technique | Best Accuracy (%) |
|---|---|---:|
| M1_LogReg | ... | ... |
| M2_RandomForest | ... | ... |
| M3_GradBoost | ... | ... |
| M4_SVC | ... | ... |
| M5_KNN | ... | ... |

---

## 5) Result Graph(s)

### 5.1 Heatmap-style Accuracy Visualization
We visualize the full grid (5×5) as an image-like matrix using matplotlib `imshow`.

### 5.2 Best Accuracy per Model (Bar Graph)
We also show which sampler gives the best accuracy for each model.

(These graphs are generated by the notebook cell provided below.)

---

## 6) How to Run (Colab)
1. Open the notebook in Google Colab.
2. Run all cells.
3. The notebook will generate:
   - `Creditcard_data_balanced_smote.csv` (balanced dataset artifact)
   - `sampling_model_accuracy_table.csv`
   - `best_sampler_per_model.csv`
   - `best_model_per_sampler.csv`
4. Upload notebook + README + generated CSVs to GitHub.

---

## 7) Files in Repository
- `Sampling_Assignment.ipynb` (main colab notebook)
- `README.md` (this file)
- `sampling_model_accuracy_table.csv`
- `best_sampler_per_model.csv`
- `best_model_per_sampler.csv`
- (optional) `Creditcard_data_balanced_smote.csv`

---

## 8) Discussion / Interpretation (What the results mean)
- **Under-sampling** can improve minority detection but may reduce overall signal by dropping majority data.
- **Over-sampling (Random/SMOTE/ADASYN)** often helps models learn minority patterns better.
- **Hybrid methods (SMOTEENN)** can help when the dataset has noisy boundaries by cleaning ambiguous points.
- Model behavior differs:
  - **Tree models (RF/GB)** often handle imbalance better than distance-based models, but still benefit from sampling.
  - **SVM/KNN** typically improve significantly when scaling + sampling are used together.

✅ Final takeaway: The best approach is usually **model-specific**, which is why we report the **best sampler per model**.

---
